nn.aliases.rst

torch.nn

torch.nn

These are the basic building blocks for graphs:

torch.nn

torch.nn

~parameter.Buffer ~parameter.Parameter ~parameter.UninitializedParameter
~parameter.UninitializedBuffer

Containers

Module Sequential ModuleList ModuleDict ParameterList ParameterDict

Global Hooks For Module

torch.nn.modules.module

register_module_forward_pre_hook register_module_forward_hook
register_module_backward_hook register_module_full_backward_pre_hook
register_module_full_backward_hook
register_module_buffer_registration_hook
register_module_module_registration_hook
register_module_parameter_registration_hook

torch

Convolution Layers

nn.Conv1d nn.Conv2d nn.Conv3d nn.ConvTranspose1d nn.ConvTranspose2d
nn.ConvTranspose3d nn.LazyConv1d nn.LazyConv2d nn.LazyConv3d
nn.LazyConvTranspose1d nn.LazyConvTranspose2d nn.LazyConvTranspose3d
nn.Unfold nn.Fold

Pooling layers

nn.MaxPool1d nn.MaxPool2d nn.MaxPool3d nn.MaxUnpool1d nn.MaxUnpool2d
nn.MaxUnpool3d nn.AvgPool1d nn.AvgPool2d nn.AvgPool3d
nn.FractionalMaxPool2d nn.FractionalMaxPool3d nn.LPPool1d nn.LPPool2d
nn.LPPool3d nn.AdaptiveMaxPool1d nn.AdaptiveMaxPool2d
nn.AdaptiveMaxPool3d nn.AdaptiveAvgPool1d nn.AdaptiveAvgPool2d
nn.AdaptiveAvgPool3d

Padding Layers

nn.ReflectionPad1d nn.ReflectionPad2d nn.ReflectionPad3d
nn.ReplicationPad1d nn.ReplicationPad2d nn.ReplicationPad3d nn.ZeroPad1d
nn.ZeroPad2d nn.ZeroPad3d nn.ConstantPad1d nn.ConstantPad2d
nn.ConstantPad3d nn.CircularPad1d nn.CircularPad2d nn.CircularPad3d

Non-linear Activations (weighted sum, nonlinearity)

nn.ELU nn.Hardshrink nn.Hardsigmoid nn.Hardtanh nn.Hardswish
nn.LeakyReLU nn.LogSigmoid nn.MultiheadAttention nn.PReLU nn.ReLU
nn.ReLU6 nn.RReLU nn.SELU nn.CELU nn.GELU nn.Sigmoid nn.SiLU nn.Mish
nn.Softplus nn.Softshrink nn.Softsign nn.Tanh nn.Tanhshrink nn.Threshold
nn.GLU

Non-linear Activations (other)

nn.Softmin nn.Softmax nn.Softmax2d nn.LogSoftmax
nn.AdaptiveLogSoftmaxWithLoss

Normalization Layers

nn.BatchNorm1d nn.BatchNorm2d nn.BatchNorm3d nn.LazyBatchNorm1d
nn.LazyBatchNorm2d nn.LazyBatchNorm3d nn.GroupNorm nn.SyncBatchNorm
nn.InstanceNorm1d nn.InstanceNorm2d nn.InstanceNorm3d
nn.LazyInstanceNorm1d nn.LazyInstanceNorm2d nn.LazyInstanceNorm3d
nn.LayerNorm nn.LocalResponseNorm nn.RMSNorm

Recurrent Layers

nn.RNNBase nn.RNN nn.LSTM nn.GRU nn.RNNCell nn.LSTMCell nn.GRUCell

Transformer Layers

nn.Transformer nn.TransformerEncoder nn.TransformerDecoder
nn.TransformerEncoderLayer nn.TransformerDecoderLayer

Linear Layers

nn.Identity nn.Linear nn.Bilinear nn.LazyLinear

Dropout Layers

nn.Dropout nn.Dropout1d nn.Dropout2d nn.Dropout3d nn.AlphaDropout
nn.FeatureAlphaDropout

Sparse Layers

nn.Embedding nn.EmbeddingBag

Distance Functions

nn.CosineSimilarity nn.PairwiseDistance

Loss Functions

nn.L1Loss nn.MSELoss nn.CrossEntropyLoss nn.CTCLoss nn.NLLLoss
nn.PoissonNLLLoss nn.GaussianNLLLoss nn.KLDivLoss nn.BCELoss
nn.BCEWithLogitsLoss nn.MarginRankingLoss nn.HingeEmbeddingLoss
nn.MultiLabelMarginLoss nn.HuberLoss nn.SmoothL1Loss nn.SoftMarginLoss
nn.MultiLabelSoftMarginLoss nn.CosineEmbeddingLoss nn.MultiMarginLoss
nn.TripletMarginLoss nn.TripletMarginWithDistanceLoss

Vision Layers

nn.PixelShuffle nn.PixelUnshuffle nn.Upsample nn.UpsamplingNearest2d
nn.UpsamplingBilinear2d

Shuffle Layers

nn.ChannelShuffle

DataParallel Layers (multi-GPU, distributed)

torch.nn.parallel

torch

nn.DataParallel nn.parallel.DistributedDataParallel

Utilities

torch.nn.utils

From the torch.nn.utils module:

Utility functions to clip parameter gradients.

torch.nn.utils

clip_grad_norm clip_grad_norm clip_grad_value get_total_norm
clip_grads_with_norm

Utility functions to flatten and unflatten Module parameters to and from
a single vector.

parameters_to_vector vector_to_parameters

Utility functions to fuse Modules with BatchNorm modules.

fuse_conv_bn_eval fuse_conv_bn_weights fuse_linear_bn_eval
fuse_linear_bn_weights

Utility functions to convert Module parameter memory formats.

convert_conv2d_weight_memory_format convert_conv3d_weight_memory_format

Utility functions to apply and remove weight normalization from Module
parameters.

weight_norm remove_weight_norm spectral_norm remove_spectral_norm

Utility functions for initializing Module parameters.

skip_init

Utility classes and functions for pruning Module parameters.

prune.BasePruningMethod prune.PruningContainer prune.Identity
prune.RandomUnstructured prune.L1Unstructured prune.RandomStructured
prune.LnStructured prune.CustomFromMask prune.identity
prune.random_unstructured prune.l1_unstructured prune.random_structured
prune.ln_structured prune.global_unstructured prune.custom_from_mask
prune.remove prune.is_pruned

Parametrizations implemented using the new parametrization functionality
in torch.nn.utils.parameterize.register_parametrization.

parametrizations.orthogonal parametrizations.weight_norm
parametrizations.spectral_norm

Utility functions to parametrize Tensors on existing Modules. Note that
these functions can be used to parametrize a given Parameter or Buffer
given a specific function that maps from an input space to the
parametrized space. They are not parameterizations that would transform
an object into a parameter. See the Parametrizations tutorial for more
information on how to implement your own parametrizations.

parametrize.register_parametrization parametrize.remove_parametrizations
parametrize.cached parametrize.is_parametrized
parametrize.transfer_parametrizations_and_params
parametrize.type_before_parametrizations

parametrize.ParametrizationList

Utility functions to call a given Module in a stateless manner.

stateless.functional_call

Utility functions in other modules

torch

nn.utils.rnn.PackedSequence nn.utils.rnn.pack_padded_sequence
nn.utils.rnn.pad_packed_sequence nn.utils.rnn.pad_sequence
nn.utils.rnn.pack_sequence nn.utils.rnn.unpack_sequence
nn.utils.rnn.unpad_sequence nn.utils.rnn.invert_permutation
nn.parameter.is_lazy nn.factory_kwargs

nn.modules.flatten.Flatten nn.modules.flatten.Unflatten

Quantized Functions

Quantization refers to techniques for performing computations and
storing tensors at lower bitwidths than floating point precision.
PyTorch supports both per tensor and per channel asymmetric linear
quantization. To learn more how to use quantized functions in PyTorch,
please refer to the quantization-doc documentation.

Lazy Modules Initialization

torch

nn.modules.lazy.LazyModuleMixin
