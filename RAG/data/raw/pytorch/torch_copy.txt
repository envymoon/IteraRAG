torch

torch

torch

Tensors

is_tensor is_storage is_complex is_conj is_floating_point is_nonzero
set_default_dtype get_default_dtype set_default_device
get_default_device set_default_tensor_type numel set_printoptions
set_flush_denormal

Creation Ops

Note

Random sampling creation ops are listed under random-sampling and
include: torch.rand torch.rand_like torch.randn torch.randn_like
torch.randint torch.randint_like torch.randperm You may also use
torch.empty with the inplace-random-sampling methods to create
torch.Tensor s with values sampled from a broader range of
distributions.

tensor sparse_coo_tensor sparse_csr_tensor sparse_csc_tensor
sparse_bsr_tensor sparse_bsc_tensor asarray as_tensor as_strided
from_file from_numpy from_dlpack frombuffer zeros zeros_like ones
ones_like arange range linspace logspace eye empty empty_like
empty_strided full full_like quantize_per_tensor quantize_per_channel
dequantize complex polar heaviside

Indexing, Slicing, Joining, Mutating Ops

adjoint argwhere cat concat concatenate conj chunk dsplit column_stack
dstack gather hsplit hstack index_add index_copy index_reduce
index_select masked_select movedim moveaxis narrow narrow_copy nonzero
permute reshape row_stack select scatter diagonal_scatter select_scatter
slice_scatter scatter_add scatter_reduce segment_reduce split squeeze
stack swapaxes swapdims t take take_along_dim tensor_split tile
transpose unbind unravel_index unsqueeze vsplit vstack where

Accelerators

Within the PyTorch repo, we define an "Accelerator" as a torch.device
that is being used alongside a CPU to speed up computation. These
devices use an asynchronous execution scheme, using torch.Stream and
torch.Event as their main way to perform synchronization. We also assume
that only one such accelerator can be available at once on a given host.
This allows us to use the current accelerator as the default device for
relevant concepts such as pinned memory, Stream device_type, FSDP, etc.

As of today, accelerator devices are (in no particular order)
"CUDA" <cuda>, "MTIA" <mtia>, "XPU" <xpu>, "MPS" <mps>, "HPU", and
PrivateUse1 (many device not in the PyTorch repo itself).

Many tools in the PyTorch Ecosystem use fork to create subprocesses (for
example dataloading or intra-op parallelism), it is thus important to
delay as much as possible any operation that would prevent further
forks. This is especially important here as most accelerator's
initialization has such effect. In practice, you should keep in mind
that checking torch.accelerator.current_accelerator is a compile-time
check by default, it is thus always fork-safe. On the contrary, passing
the check_available=True flag to this function or calling
torch.accelerator.is_available() will usually prevent later fork.

Some backends provide an experimental opt-in option to make the runtime
availability check fork-safe. When using the CUDA device
PYTORCH_NVML_BASED_CUDA_CHECK=1 can be used for example.

Stream Event

Generators

Generator

Random sampling

seed manual_seed initial_seed get_rng_state set_rng_state

torch.default_generator

bernoulli multinomial normal poisson rand rand_like randint randint_like
randn randn_like randperm

In-place random sampling

There are a few more in-place random sampling functions defined on
Tensors as well. Click through to refer to their documentation:

- torch.Tensor.bernoulli_ - in-place version of torch.bernoulli
- torch.Tensor.cauchy_ - numbers drawn from the Cauchy distribution
- torch.Tensor.exponential_ - numbers drawn from the exponential
  distribution
- torch.Tensor.geometric_ - elements drawn from the geometric
  distribution
- torch.Tensor.log_normal_ - samples from the log-normal distribution
- torch.Tensor.normal_ - in-place version of torch.normal
- torch.Tensor.random_ - numbers sampled from the discrete uniform
  distribution
- torch.Tensor.uniform_ - numbers sampled from the continuous uniform
  distribution

Quasi-random sampling

quasirandom.SobolEngine

Serialization

save load

Parallelism

get_num_threads set_num_threads get_num_interop_threads
set_num_interop_threads

Locally disabling gradient computation

The context managers torch.no_grad, torch.enable_grad, and
torch.set_grad_enabled are helpful for locally disabling and enabling
gradient computation. See locally-disable-grad for more details on their
usage. These context managers are thread local, so they won't work if
you send work to another thread using the threading module, etc.

Examples:

    >>> x = torch.zeros(1, requires_grad=True)
    >>> with torch.no_grad():
    ...     y = x * 2
    >>> y.requires_grad
    False

    >>> is_train = False
    >>> with torch.set_grad_enabled(is_train):
    ...     y = x * 2
    >>> y.requires_grad
    False

    >>> torch.set_grad_enabled(True)  # this can also be used as a function
    >>> y = x * 2
    >>> y.requires_grad
    True

    >>> torch.set_grad_enabled(False)
    >>> y = x * 2
    >>> y.requires_grad
    False

no_grad enable_grad autograd.grad_mode.set_grad_enabled is_grad_enabled
autograd.grad_mode.inference_mode is_inference_mode_enabled

Math operations

Constants

  ----- ----------------------------------------------------------------------------------------------
  inf   A floating-point positive infinity. Alias for math.inf.
  nan   A floating-point "not a number" value. This value is not a legal number. Alias for math.nan.
  ----- ----------------------------------------------------------------------------------------------

Pointwise Ops

abs absolute acos arccos acosh arccosh add addcdiv addcmul angle asin
arcsin asinh arcsinh atan arctan atanh arctanh atan2 arctan2 bitwise_not
bitwise_and bitwise_or bitwise_xor bitwise_left_shift
bitwise_right_shift ceil clamp clip conj_physical copysign cos cosh
deg2rad div divide digamma erf erfc erfinv exp exp2 expm1
fake_quantize_per_channel_affine fake_quantize_per_tensor_affine fix
float_power floor floor_divide fmod frac frexp gradient imag ldexp lerp
lgamma log log10 log1p log2 logaddexp logaddexp2 logical_and logical_not
logical_or logical_xor logit hypot i0 igamma igammac mul multiply
mvlgamma nan_to_num neg negative nextafter polygamma positive pow
quantized_batch_norm quantized_max_pool1d quantized_max_pool2d rad2deg
real reciprocal remainder round rsqrt sigmoid sign sgn signbit sin sinc
sinh softmax sqrt square sub subtract tan tanh true_divide trunc xlogy

Reduction Ops

argmax argmin amax amin aminmax all any max min dist logsumexp mean
nanmean median nanmedian mode norm nansum prod quantile nanquantile std
std_mean sum unique unique_consecutive var var_mean count_nonzero
hash_tensor

Comparison Ops

allclose argsort eq equal ge greater_equal gt greater isclose isfinite
isin isinf isposinf isneginf isnan isreal kthvalue le less_equal lt less
maximum minimum fmax fmin ne not_equal sort topk msort

Spectral Ops

stft istft bartlett_window blackman_window hamming_window hann_window
kaiser_window

Other Operations

atleast_1d atleast_2d atleast_3d bincount block_diag broadcast_tensors
broadcast_to broadcast_shapes bucketize cartesian_prod cdist clone
combinations corrcoef cov cross cummax cummin cumprod cumsum diag
diag_embed diagflat diagonal diff einsum flatten flip fliplr flipud kron
rot90 gcd histc histogram histogramdd meshgrid lcm logcumsumexp ravel
renorm repeat_interleave roll searchsorted tensordot trace tril
tril_indices triu triu_indices unflatten vander view_as_real
view_as_complex resolve_conj resolve_neg

BLAS and LAPACK Operations

addbmm addmm addmv addr baddbmm bmm chain_matmul cholesky
cholesky_inverse cholesky_solve dot geqrf ger inner inverse det logdet
slogdet lu lu_solve lu_unpack matmul matrix_power matrix_exp mm mv orgqr
ormqr outer pinverse qr svd svd_lowrank pca_lowrank lobpcg trapz
trapezoid cumulative_trapezoid triangular_solve vdot

Foreach Operations

Warning

This API is in beta and subject to future changes. Forward-mode AD is
not supported.

foreach_abs foreach_abs_ foreach_acos foreach_acos_ foreach_asin
foreach_asin_ foreach_atan foreach_atan_ foreach_ceil foreach_ceil_
foreach_cos foreach_cos_ foreach_cosh foreach_cosh_ foreach_erf
foreach_erf_ foreach_erfc foreach_erfc_ foreach_exp foreach_exp_
foreach_expm1 foreach_expm1_ foreach_floor foreach_floor_ foreach_log
foreach_log_ foreach_log10 foreach_log10_ foreach_log1p foreach_log1p_
foreach_log2 foreach_log2_ foreach_neg foreach_neg_ foreach_tan
foreach_tan_ foreach_sin foreach_sin_ foreach_sinh foreach_sinh_
foreach_round foreach_round_ foreach_sqrt foreach_sqrt_ foreach_lgamma
foreach_lgamma_ foreach_frac foreach_frac_ foreach_reciprocal
foreach_reciprocal_ foreach_sigmoid foreach_sigmoid_ foreach_trunc
foreach_trunc_ foreach_zero_

Utilities

compiled_with_cxx11_abi result_type can_cast promote_types
use_deterministic_algorithms are_deterministic_algorithms_enabled
is_deterministic_algorithms_warn_only_enabled
set_deterministic_debug_mode get_deterministic_debug_mode
set_float32_matmul_precision get_float32_matmul_precision
set_warn_always get_device_module is_warn_always_enabled vmap assert

Symbolic Numbers

SymInt

SymFloat

SymBool

sym_float sym_fresh_size sym_int sym_max sym_min sym_not sym_ite sym_sum

Export Path

Warning

This feature is a prototype and may have compatibility breaking changes
in the future.

export generated/exportdb/index

Control Flow

Warning

This feature is a prototype and may have compatibility breaking changes
in the future.

cond

Optimizations

compile

torch.compile documentation

Operator Tags

Tag

torch.aliases.md
