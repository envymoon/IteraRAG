{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc6e321-4d4b-43f5-9860-cd14f29e0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "pubmed_ds = load_dataset(\"csv\", data_files=\"../data/raw/PubMed/train.csv\")\n",
    "\n",
    "def process_pubmed_row(examples):\n",
    "    processed_chunks = []\n",
    "    for i in range(len(examples['abstract_text'])):\n",
    "\n",
    "        text_content = f\"In PubMed abstract {examples['abstract_id'][i]}, the {examples['target'][i]} section states: {examples['abstract_text'][i]}\"\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": \"PubMed\",\n",
    "            \"abstract_id\": examples['abstract_id'][i],\n",
    "            \"target\": examples['target'][i],\n",
    "            \"line_number\": examples['line_number'][i]\n",
    "        }\n",
    "        \n",
    "        processed_chunks.append({\n",
    "            \"content\": text_content,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "    \n",
    "    return {\"processed_data\": processed_chunks}\n",
    "\n",
    "pubmed_processed = pubmed_ds.map(process_pubmed_row, batched=True, remove_columns=pubmed_ds['train'].column_names)\n",
    "pubmed_chunks = list(pubmed_processed['train']['processed_data'])\n",
    "\n",
    "# Recursive Character Splitting\n",
    "def txt_chunking(text, chunk_size=600, metadata=None):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        if len(current_chunk) + len(p) <= chunk_size:\n",
    "            current_chunk += p + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append({\n",
    "                    \"content\": current_chunk.strip(),\n",
    "                    \"metadata\": metadata.copy() if metadata else {}\n",
    "                })\n",
    "            current_chunk = p + \"\\n\\n\"\n",
    "            \n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            \"content\": current_chunk.strip(),\n",
    "            \"metadata\": metadata.copy() if metadata else {}\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "txt_chunks = [] \n",
    "def load_txt_folder(folder_path, source_name):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "    \n",
    "                meta = {\n",
    "                    \"source\": source_name,\n",
    "                    \"file_name\": filename\n",
    "                }\n",
    "                \n",
    "                file_chunks = txt_chunking(content, metadata=meta)\n",
    "                txt_chunks.extend(file_chunks)\n",
    "\n",
    "\n",
    "load_txt_folder(\"../data/raw/libc\", \"libc\")\n",
    "load_txt_folder(\"../data/raw/pytorch\", \"pytorch\")\n",
    "\n",
    "final_chunks = txt_chunks + pubmed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6274530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Use a local embedding model to calculate semantic similarity for metrics like Answer Relevance.\n",
    "test_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da3ec04-ec9a-427e-8ea3-0e6f29c77857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = 384 \n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "text_list = [c['content'] for c in final_chunks]\n",
    "embeddings = test_embeddings.embed_documents(text_list) \n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "index.add(embeddings)\n",
    "\n",
    "def query_rag_v1(user_query, k):\n",
    "    query_vec = test_embeddings.embed_query(user_query)\n",
    "    query_vec = np.array([query_vec]).astype('float32')\n",
    "    \n",
    "    _, I_vector = index.search(query_vec, k)\n",
    "    vector_indices = I_vector[0].tolist()\n",
    "    \n",
    "    retrieved_texts = [final_chunks[i]['content'] for i in vector_indices if i != -1]\n",
    "    \n",
    "    context_str = \"\\n\\n\".join(retrieved_texts)\n",
    "    full_prompt = f\"Context:\\n{context_str}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    \n",
    "    return full_prompt, retrieved_texts\n",
    "\n",
    "def query_rag_final_v1(user_query, k=10):\n",
    "    full_prompt, contexts = query_rag_v1(user_query, k)\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model_llm.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_llm.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return answer, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a794e737-375a-4eaf-b280-6136218e0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89e7ee3-6587-44da-8e6a-878ab4a8d983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2767935c584efcb3dca22ddd7a9932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "with open('../data/raw/questions.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines[1:]:\n",
    "    parts = line.strip().split(',', 2) \n",
    "    if len(parts) == 3:\n",
    "        data.append(parts)\n",
    "\n",
    "df_test = pd.DataFrame(data, columns=['category', 'question', 'ground_truth'])\n",
    "\n",
    "df_test['ground_truth'] = df_test['ground_truth'].str.strip('\"')\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    q = row['question']\n",
    "    gt = row['ground_truth']\n",
    "    \n",
    "    ans, contexts = query_rag_final_v1(q)\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": ans,\n",
    "        \"contexts\": [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in contexts],  \n",
    "        \"ground_truth\": gt\n",
    "    })\n",
    "\n",
    "eval_dataset = Dataset.from_list(results) \n",
    "\n",
    "with open(\"../data/eval/results_v1.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "del model_llm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce56636d-5611-42d2-8fe2-f5c5324c65fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0664516c986647c6a2be5b3889a0fea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@k': np.float64(0.5612244897959183), 'MRR': np.float64(0.5948250728862974), 'Avg Context-GT Sim': np.float64(0.7944092929327524), 'Avg Answer-GT Sim': np.float64(0.7496613243003292), 'Avg Answer-Context Sim': np.float64(0.858734216109723), 'Hallucination Rate': np.float64(0.01020408163265306)}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embed(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    return test_embeddings.embed_documents(texts)\n",
    "\n",
    "def recall_at_k(contexts, gt, threshold=0.75):\n",
    "    gt_emb = embed(gt)[0]\n",
    "    ctx_embs = embed(contexts)\n",
    "    sims = cosine_similarity([gt_emb], ctx_embs)[0]\n",
    "    hit = np.max(sims) > threshold\n",
    "    best_rank = int(np.argmax(sims)) + 1\n",
    "    return hit, best_rank, float(np.max(sims))\n",
    "\n",
    "def answer_gt_similarity(ans, gt):\n",
    "    emb = embed([ans, gt])\n",
    "    sim = cosine_similarity([emb[0]], [emb[1]])[0][0]\n",
    "    return float(sim)\n",
    "    \n",
    "def answer_context_similarity(ans, contexts):\n",
    "    ans_emb = embed(ans)[0]\n",
    "    ctx_embs = embed(contexts)\n",
    "    sims = cosine_similarity([ans_emb], ctx_embs)[0]\n",
    "    return float(np.max(sims))\n",
    "\n",
    "def hallucination_flag(ans_gt_sim, ans_ctx_sim,\n",
    "                       gt_th=0.6, ctx_th=0.6):\n",
    "    if ans_gt_sim < gt_th and ans_ctx_sim < ctx_th:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "with open(\"../data/eval/results_v1.json\") as f:\n",
    "   results = json.load(f)\n",
    "\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for item in tqdm(results):\n",
    "    q = item[\"question\"]\n",
    "    ans = item[\"answer\"]\n",
    "    contexts = item[\"contexts\"]\n",
    "    gt = item[\"ground_truth\"]\n",
    "\n",
    "    hit, rank, ctx_gt_sim = recall_at_k(contexts, gt)\n",
    "    ans_gt_sim = answer_gt_similarity(ans, gt)\n",
    "    ans_ctx_sim = answer_context_similarity(ans, contexts)\n",
    "    hallucinated = hallucination_flag(ans_gt_sim, ans_ctx_sim)\n",
    "\n",
    "    eval_results.append({\n",
    "        \"question\": q,\n",
    "        \"hit\": hit,\n",
    "        \"best_rank\": rank,\n",
    "        \"ctx_gt_sim\": ctx_gt_sim,\n",
    "        \"ans_gt_sim\": ans_gt_sim,\n",
    "        \"ans_ctx_sim\": ans_ctx_sim,\n",
    "        \"hallucinated\": hallucinated\n",
    "    })\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(eval_results)\n",
    "\n",
    "summary = {\n",
    "    \"Recall@k\": df[\"hit\"].mean(),\n",
    "    \"MRR\": (1 / df[\"best_rank\"]).mean(),\n",
    "    \"Avg Context-GT Sim\": df[\"ctx_gt_sim\"].mean(),\n",
    "    \"Avg Answer-GT Sim\": df[\"ans_gt_sim\"].mean(),\n",
    "    \"Avg Answer-Context Sim\": df[\"ans_ctx_sim\"].mean(),\n",
    "    \"Hallucination Rate\": df[\"hallucinated\"].mean()\n",
    "}\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(\"../data/eval/summary_v1.json\", \"w\") as f:\n",
    "    json.dump(summary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch310)",
   "language": "python",
   "name": "torch310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
