{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582ae00-e1af-4731-ab41-d435c7ed8213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "pubmed_ds = load_dataset(\"csv\", data_files=\"../data/raw/PubMed/train.csv\")\n",
    "\n",
    "def process_pubmed_row(examples):\n",
    "    processed_chunks = []\n",
    "    for i in range(len(examples['abstract_text'])):\n",
    "\n",
    "        text_content = f\"In PubMed abstract {examples['abstract_id'][i]}, the {examples['target'][i]} section states: {examples['abstract_text'][i]}\"\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": \"PubMed\",\n",
    "            \"abstract_id\": examples['abstract_id'][i],\n",
    "            \"target\": examples['target'][i],\n",
    "            \"line_number\": examples['line_number'][i]\n",
    "        }\n",
    "        \n",
    "        processed_chunks.append({\n",
    "            \"content\": text_content,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "    \n",
    "    return {\"processed_data\": processed_chunks}\n",
    "\n",
    "pubmed_processed = pubmed_ds.map(process_pubmed_row, batched=True, remove_columns=pubmed_ds['train'].column_names)\n",
    "pubmed_chunks = list(pubmed_processed['train']['processed_data'])\n",
    "\n",
    "# Recursive Character Splitting\n",
    "def txt_chunking(text, chunk_size=600, overlap=100, metadata=None):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        if len(current_chunk) + len(p) <= chunk_size:\n",
    "            current_chunk += p + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append({\n",
    "                    \"content\": current_chunk.strip(),\n",
    "                    \"metadata\": metadata.copy() if metadata else {}\n",
    "                })\n",
    "            current_chunk = current_chunk[-overlap:] + p + \"\\n\\n\"\n",
    "   \n",
    "    # optimized on overlap  \n",
    "    if current_chunk:\n",
    "        chunks.append({\"content\": current_chunk.strip(), \"metadata\": metadata})\n",
    "    \n",
    "        raw_overlap = current_chunk[-overlap:]\n",
    "        first_space = raw_overlap.find(' ')\n",
    "        if first_space != -1:\n",
    "            current_chunk = raw_overlap[first_space:].strip() + \"\\n\\n\" + p + \"\\n\\n\"\n",
    "        else:\n",
    "            current_chunk = raw_overlap + \"\\n\\n\" + p + \"\\n\\n\"\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "txt_chunks = [] \n",
    "def load_txt_folder(folder_path, source_name):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "    \n",
    "                meta = {\n",
    "                    \"source\": source_name,\n",
    "                    \"file_name\": filename\n",
    "                }\n",
    "                \n",
    "                file_chunks = txt_chunking(content, metadata=meta)\n",
    "                txt_chunks.extend(file_chunks)\n",
    "\n",
    "\n",
    "load_txt_folder(\"../data/raw/libc\", \"libc\")\n",
    "load_txt_folder(\"../data/raw/pytorch\", \"pytorch\")\n",
    "\n",
    "final_chunks = txt_chunks + pubmed_chunks\n",
    "\n",
    "pubmed_map = {}\n",
    "for chunk in final_chunks:\n",
    "    if chunk['metadata'].get('source') == 'PubMed':\n",
    "        abs_id = chunk['metadata']['abstract_id']             \n",
    "        if abs_id not in pubmed_map:\n",
    "            pubmed_map[abs_id] = []\n",
    "        pubmed_map[abs_id].append(chunk) \n",
    "\n",
    "def expand_context(retrieved_chunks, pubmed_map, window_size=1):\n",
    "    expanded_results = []\n",
    "    for hit in retrieved_chunks:\n",
    "        meta = hit['metadata']\n",
    "        if meta.get('source') != 'PubMed':\n",
    "            expanded_results.append(hit['content'])\n",
    "            continue\n",
    "            \n",
    "        abs_id = meta['abstract_id']\n",
    "        curr_line = meta['line_number']\n",
    "        \n",
    "        all_lines = pubmed_map.get(abs_id, [])\n",
    "        neighbors = [\n",
    "            c['content'] for c in all_lines \n",
    "            if abs(c['metadata']['line_number'] - curr_line) <= window_size\n",
    "        ]\n",
    "        expanded_results.append(\" \".join(neighbors))\n",
    "    return expanded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879c16c-d69a-4548-9ea9-be69a9794dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Use a local embedding model to calculate semantic similarity\n",
    "test_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea4de2-278a-49b2-864d-7e95f419a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b28b1b-bc34-41cb-ba81-01bc0d02eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "text_list = [c['content'] for c in final_chunks]\n",
    "embeddings = test_embeddings.embed_documents(text_list) \n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "d = embeddings.shape[1]\n",
    "M = 32\n",
    "index = faiss.IndexHNSWFlat(d, M)\n",
    "index.hnsw.efConstruction = 200\n",
    "index.add(embeddings)\n",
    "\n",
    "tokenized_corpus = [chunk['content'].lower().split() for chunk in final_chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "\n",
    "# RRF Fusion Algorithm\n",
    "def rrf_fusion(vector_results, bm25_results, k=60):\n",
    "    scores = {}\n",
    "    for rank, idx in enumerate(vector_results):\n",
    "        scores[idx] = scores.get(idx, 0) + 1 / (k + rank)\n",
    "    for rank, idx in enumerate(bm25_results):\n",
    "        scores[idx] = scores.get(idx, 0) + 1 / (k + rank)\n",
    "    \n",
    "    sorted_indices = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n",
    "    return sorted_indices\n",
    "\n",
    "reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cpu')\n",
    "\n",
    "# Tag Mapping for PubMed data\n",
    "target_to_indices = {}\n",
    "for idx, chunk in enumerate(final_chunks):\n",
    "    t = chunk['metadata'].get('target')\n",
    "    if t:\n",
    "        if t not in target_to_indices:\n",
    "            target_to_indices[t] = []\n",
    "        target_to_indices[t].append(idx)\n",
    "\n",
    "\n",
    "# Hybrid Search\n",
    "def query_rag_v3(user_query, k, target_filter=None, rerank_top_n=5):\n",
    "    query_vec = test_embeddings.embed_query(user_query)\n",
    "    query_vec = np.array([query_vec]).astype('float32')\n",
    "    \n",
    "    # Hard Filter\n",
    "    allowed_indices = target_to_indices.get(target_filter, None) if target_filter else None\n",
    "    \n",
    "    # FAISS Retrieval\n",
    "    if allowed_indices is not None:\n",
    "        selector = faiss.IDSelectorBatch(allowed_indices)\n",
    "        params = faiss.SearchParameters(sel=selector)\n",
    "        index.hnsw.efSearch = 128\n",
    "        _, I_vector = index.search(query_vec, k * 2, params=params)\n",
    "    else:\n",
    "        index.hnsw.efSearch = 128\n",
    "        _, I_vector = index.search(query_vec, k * 2)\n",
    "    \n",
    "    vector_indices = I_vector[0].tolist()\n",
    "\n",
    "    # BM25 Retrieval\n",
    "    tokenized_query = user_query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    if allowed_indices is not None:\n",
    "        mask = np.zeros_like(bm25_scores)\n",
    "        mask[allowed_indices] = 1\n",
    "        bm25_scores = bm25_scores * mask\n",
    "\n",
    "    bm25_indices = np.argsort(bm25_scores)[::-1][:k * 2].tolist()\n",
    "\n",
    "    # RRF\n",
    "    combined_indices = rrf_fusion(vector_indices, bm25_indices)\n",
    "\n",
    "    # Reranking\n",
    "    candidate_chunks = [final_chunks[i] for i in combined_indices[:10]]\n",
    "    pairs = [[user_query, c['content']] for c in candidate_chunks]\n",
    "    rerank_scores = reranker_model.predict(pairs)\n",
    "    reranked_chunks = [c for _, c in sorted(zip(rerank_scores, candidate_chunks), key=lambda x: x[0], reverse=True)]\n",
    "    \n",
    "    top_chunks = reranked_chunks[:rerank_top_n]\n",
    "    # context expansion\n",
    "    expanded_texts = expand_context(top_chunks, pubmed_map, window_size=1)\n",
    "    context_str = \"\\n\\n \".join(expanded_texts)\n",
    "    full_prompt = f\"You are a helpful agent, answer the question based on provided contexts.\\nContext:\\n{context_str}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "\n",
    "    \n",
    "    return full_prompt, expanded_texts\n",
    "\n",
    "def query_rag_final_v3(user_query, k=30):\n",
    "    full_prompt, expanded_texts = query_rag_v3(user_query, k) \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model_llm.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_llm.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return answer, expanded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed96dc-6616-4504-9a43-f645782ddc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "questions = []\n",
    "with open('../data/raw/questions.txt', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        q = row.get('Question ', row.get('Question')).strip()\n",
    "        gt = row.get('Ground Truth', row.get('ground_truth')).strip()\n",
    "        if q:\n",
    "            questions.append({\"question\": q, \"ground_truth\": gt})\n",
    "\n",
    "results = []\n",
    "for item in tqdm(questions):\n",
    "    q = item['question']\n",
    "    gt = item['ground_truth']\n",
    "    \n",
    "    ans, contexts = query_rag_final_v3(q)\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": ans,\n",
    "        \"contexts\": contexts,  \n",
    "        \"ground_truth\": gt\n",
    "    })\n",
    "\n",
    "\n",
    "with open(\"../data/eval/results_v3.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "del model_llm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc050b4-d0bd-4ab5-b34b-3ff5c99c9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embed(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    return test_embeddings.embed_documents(texts)\n",
    "\n",
    "def recall_at_k(contexts, gt, threshold=0.75):\n",
    "    gt_emb = embed(gt)[0]\n",
    "    ctx_embs = embed(contexts)\n",
    "    sims = cosine_similarity([gt_emb], ctx_embs)[0]\n",
    "    hit = np.max(sims) > threshold\n",
    "    best_rank = int(np.argmax(sims)) + 1\n",
    "    return hit, best_rank, float(np.max(sims))\n",
    "\n",
    "def answer_gt_similarity(ans, gt):\n",
    "    emb = embed([ans, gt])\n",
    "    sim = cosine_similarity([emb[0]], [emb[1]])[0][0]\n",
    "    return float(sim)\n",
    "    \n",
    "def answer_context_similarity(ans, contexts):\n",
    "    ans_emb = embed(ans)[0]\n",
    "    ctx_embs = embed(contexts)\n",
    "    sims = cosine_similarity([ans_emb], ctx_embs)[0]\n",
    "    return float(np.max(sims))\n",
    "\n",
    "def hallucination_flag(ans_gt_sim, ans_ctx_sim,\n",
    "                       gt_th=0.6, ctx_th=0.6):\n",
    "    if ans_gt_sim < gt_th and ans_ctx_sim < ctx_th:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "with open(\"../data/eval/results_v4.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for item in tqdm(results):\n",
    "    q = item[\"question\"]\n",
    "    ans = item[\"answer\"]\n",
    "    contexts = item[\"contexts\"]\n",
    "    gt = item[\"ground_truth\"]\n",
    "\n",
    "    hit, rank, ctx_gt_sim = recall_at_k(contexts, gt)\n",
    "    ans_gt_sim = answer_gt_similarity(ans, gt)\n",
    "    ans_ctx_sim = answer_context_similarity(ans, contexts)\n",
    "    hallucinated = hallucination_flag(ans_gt_sim, ans_ctx_sim)\n",
    "\n",
    "    eval_results.append({\n",
    "        \"question\": q,\n",
    "        \"hit\": hit,\n",
    "        \"best_rank\": rank,\n",
    "        \"ctx_gt_sim\": ctx_gt_sim,\n",
    "        \"ans_gt_sim\": ans_gt_sim,\n",
    "        \"ans_ctx_sim\": ans_ctx_sim,\n",
    "        \"hallucinated\": hallucinated\n",
    "    })\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(eval_results)\n",
    "\n",
    "summary = {\n",
    "    \"Recall@k\": df[\"hit\"].mean(),\n",
    "    \"MRR\": (1 / df[\"best_rank\"]).mean(),\n",
    "    \"Avg Context-GT Sim\": df[\"ctx_gt_sim\"].mean(),\n",
    "    \"Avg Answer-GT Sim\": df[\"ans_gt_sim\"].mean(),\n",
    "    \"Avg Answer-Context Sim\": df[\"ans_ctx_sim\"].mean(),\n",
    "    \"Hallucination Rate\": df[\"hallucinated\"].mean()\n",
    "}\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(\"../data/eval/summary_v4.json\", \"w\") as f:\n",
    "    json.dump(summary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch310)",
   "language": "python",
   "name": "torch310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
