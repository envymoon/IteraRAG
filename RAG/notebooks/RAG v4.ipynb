{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3e10bc-3296-4fbb-b05c-4c62d0627d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "pubmed_ds = load_dataset(\"csv\", data_files=\"../data/raw/PubMed/train.csv\")\n",
    "\n",
    "def process_pubmed_row(examples):\n",
    "    processed_chunks = []\n",
    "    for i in range(len(examples['abstract_text'])):\n",
    "\n",
    "        text_content = f\"In PubMed abstract {examples['abstract_id'][i]}, the {examples['target'][i]} section states: {examples['abstract_text'][i]}\"\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": \"PubMed\",\n",
    "            \"abstract_id\": examples['abstract_id'][i],\n",
    "            \"target\": examples['target'][i],\n",
    "            \"line_number\": examples['line_number'][i]\n",
    "        }\n",
    "        \n",
    "        processed_chunks.append({\n",
    "            \"content\": text_content,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "    \n",
    "    return {\"processed_data\": processed_chunks}\n",
    "\n",
    "pubmed_processed = pubmed_ds.map(process_pubmed_row, batched=True, remove_columns=pubmed_ds['train'].column_names)\n",
    "pubmed_chunks = list(pubmed_processed['train']['processed_data'])\n",
    "\n",
    "\n",
    "def libc_semantic_chunking(text, metadata):\n",
    "    header_pattern = re.compile(r'\\n([A-Z][A-Z\\s]+)\\n')\n",
    "    \n",
    "    headers = list(header_pattern.finditer(text))\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(len(headers)):\n",
    "        start_idx = headers[i].start()\n",
    "        end_idx = headers[i+1].start() if i + 1 < len(headers) else len(text)\n",
    "        \n",
    "        section_title = headers[i].group(1).strip()\n",
    "        section_content = text[start_idx:end_idx].strip()\n",
    "        \n",
    "        meta = metadata.copy()\n",
    "        meta[\"section\"] = section_title\n",
    "        \n",
    "        chunks.append({\n",
    "            \"content\": section_content,\n",
    "            \"metadata\": meta\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def recursive_smart_chunking(text, chunk_size=600, overlap=100, metadata=None):\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    \n",
    "    def split_text(text, separators):\n",
    "        if len(text) <= chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        sep = separators[0]\n",
    "        for s in separators:\n",
    "            if s in text:\n",
    "                sep = s\n",
    "                break\n",
    "        \n",
    "        final_chunks = []\n",
    "        parts = text.split(sep)\n",
    "        current_doc = \"\"\n",
    "        \n",
    "        for p in parts:\n",
    "            if len(current_doc) + len(p) + len(sep) <= chunk_size:\n",
    "                current_doc += (sep if current_doc else \"\") + p\n",
    "            else:\n",
    "                if current_doc:\n",
    "                    final_chunks.append(current_doc)\n",
    "                current_doc = current_doc[-overlap:] + (sep if current_doc else \"\") + p\n",
    "                \n",
    "        if current_doc:\n",
    "            final_chunks.append(current_doc)\n",
    "        return final_chunks\n",
    "\n",
    "    raw_chunks = split_text(text, separators)\n",
    "    return [{\"content\": c, \"metadata\": metadata.copy() if metadata else {}} for c in raw_chunks]\n",
    "\n",
    "\n",
    "def load_and_process(folder_path, source_name):\n",
    "    txt_chunks = []\n",
    "    source_map = {} \n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            meta = {\"source\": source_name, \"file_name\": filename}\n",
    "            \n",
    "            if source_name == \"libc\":\n",
    "                file_chunks = libc_semantic_chunking(content, meta)\n",
    "            else:\n",
    "                file_chunks = recursive_smart_chunking(content, metadata=meta)\n",
    "                for i, chunk in enumerate(file_chunks):\n",
    "                    chunk['metadata']['chunk_index'] = i\n",
    "            \n",
    "            txt_chunks.extend(file_chunks)\n",
    "            source_map[filename] = file_chunks\n",
    "            \n",
    "    return txt_chunks, source_map\n",
    "\n",
    "\n",
    "libc_chunks, libc_map_raw = load_and_process(\"../data/raw/libc\", \"libc\")\n",
    "pytorch_chunks, pytorch_map = load_and_process(\"../data/raw/pytorch\", \"pytorch\")\n",
    "final_chunks = libc_chunks + pytorch_chunks + pubmed_chunks\n",
    "\n",
    "libc_map = {}\n",
    "for f_name, chunks in libc_map_raw.items():\n",
    "    libc_map[f_name] = {c['metadata']['section']: c['content'] for c in chunks}\n",
    "\n",
    "pubmed_map = {}\n",
    "for chunk in pubmed_chunks:\n",
    "    abs_id = chunk['metadata']['abstract_id']             \n",
    "    if abs_id not in pubmed_map:\n",
    "        pubmed_map[abs_id] = []\n",
    "    pubmed_map[abs_id].append(chunk)\n",
    "\n",
    "global_maps = {\n",
    "    'pubmed': pubmed_map,   \n",
    "    'pytorch': pytorch_map, \n",
    "    'libc': libc_map        \n",
    "}\n",
    "\n",
    "def get_expanded_content(hit, global_maps, window_size=1):\n",
    "    meta = hit['metadata']\n",
    "    source = meta.get('source')\n",
    "    \n",
    "    if source == 'PubMed':\n",
    "        abs_id = meta['abstract_id']\n",
    "        curr_line = meta['line_number']\n",
    "        all_lines = global_maps['pubmed'].get(abs_id, [])\n",
    "        neighbors = [\n",
    "            c['content'] for c in all_lines \n",
    "            if abs(c['metadata']['line_number'] - curr_line) <= window_size\n",
    "        ]\n",
    "        return \" \".join(neighbors)\n",
    "    \n",
    "    elif source == 'pytorch':\n",
    "        f_name = meta['file_name']\n",
    "        idx = meta['chunk_index']\n",
    "        all_chunks = global_maps['pytorch'].get(f_name, [])\n",
    "        neighbors = [\n",
    "            c['content'] for c in all_chunks \n",
    "            if abs(c['metadata']['chunk_index'] - idx) <= window_size\n",
    "        ]\n",
    "        return \"\\n\\n...[CONTEXT OVERLAP]...\\n\\n\".join(neighbors)\n",
    "    \n",
    "    elif source == 'libc':\n",
    "        f_name = meta['file_name']\n",
    "        sec = meta['section']\n",
    "        return global_maps['libc'].get(f_name, {}).get(sec, hit['content'])\n",
    "    \n",
    "    return hit['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834690c6-afc0-4c47-98fe-c5a44b3734ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Use a local embedding model to calculate semantic similarity for metrics like Answer Relevance.\n",
    "test_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1d68cc-2aee-43e2-8368-136eb91b1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "index = faiss.read_index(\"pubmed_pytorch_libc.index\")\n",
    "\n",
    "with open(\"final_chunks_metadata.pkl\", \"rb\") as f:\n",
    "    final_chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4766db43-5c29-4d1d-8907-e1e38439ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building FAISS Index: 100%|██████████| 4370/4370 [1:47:38<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index saved\n",
      "Metadata saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "batch_size = 512\n",
    "text_list = [c['content'] for c in final_chunks]\n",
    "d = 384\n",
    "M = 32\n",
    "index = faiss.IndexHNSWFlat(d, M)\n",
    "index.hnsw.efConstruction = 200\n",
    "\n",
    "for i in tqdm(range(0, len(text_list), batch_size), desc=\"Building FAISS Index\"):\n",
    "    batch_texts = text_list[i : i + batch_size]\n",
    "    batch_embeddings = test_embeddings.embed_documents(batch_texts)\n",
    "    batch_embeddings_np = np.array(batch_embeddings).astype('float32')\n",
    "    index.add(batch_embeddings_np)\n",
    "    \n",
    "    del batch_embeddings, batch_embeddings_np\n",
    "    gc.collect() \n",
    "\n",
    "faiss.write_index(index, \"pubmed_pytorch_libc.index\")\n",
    "print(\"FAISS Index saved\")\n",
    "\n",
    "with open(\"final_chunks_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_chunks, f)\n",
    "print(\"Metadata saved.\")\n",
    "\n",
    "def technical_tokenizer(text):\n",
    "    tokens = re.sub(r'[^a-zA-Z0-9._\\s]', ' ', text.lower()).split()\n",
    "    return tokens\n",
    "\n",
    "def get_tokenized_corpus(chunks):\n",
    "    for chunk in chunks:\n",
    "        yield technical_tokenizer(chunk['content'])\n",
    "\n",
    "bm25 = BM25Okapi(get_tokenized_corpus(final_chunks))\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# RRF Fusion Algorithm\n",
    "def rrf_fusion(vector_results, bm25_results, k=60):\n",
    "    scores = {}\n",
    "    for rank, idx in enumerate(vector_results):\n",
    "        scores[idx] = scores.get(idx, 0) + 1 / (k + rank)\n",
    "    for rank, idx in enumerate(bm25_results):\n",
    "        scores[idx] = scores.get(idx, 0) + 1 / (k + rank)\n",
    "    \n",
    "    sorted_indices = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n",
    "    return sorted_indices\n",
    "\n",
    "reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cpu')\n",
    "\n",
    "# Hard filter\n",
    "source_to_indices = {}\n",
    "for idx, chunk in enumerate(final_chunks):\n",
    "    s = chunk['metadata'].get('source')\n",
    "    if s:\n",
    "        if s not in source_to_indices:\n",
    "            source_to_indices[s] = []\n",
    "        source_to_indices[s].append(idx)\n",
    "\n",
    "\n",
    "# Hybrid Search\n",
    "def query_rag_v4(user_query, k, rerank_top_n, source_filter=None):\n",
    "    query_vec = test_embeddings.embed_query(user_query)\n",
    "    query_vec = np.array([query_vec]).astype('float32')\n",
    "    \n",
    "    # Hard Filter\n",
    "    allowed_indices = None\n",
    "    if source_filter:\n",
    "        allowed_indices = source_to_indices.get(source_filter)\n",
    "    \n",
    "    # FAISS Retrieval\n",
    "    index.hnsw.efSearch = 128 \n",
    "\n",
    "    search_k = k * 2\n",
    "    if allowed_indices is not None:\n",
    "        search_k = min(search_k, len(allowed_indices))\n",
    "    \n",
    "    if allowed_indices is not None and search_k > 0:\n",
    "        selector = faiss.IDSelectorBatch(allowed_indices)\n",
    "        params = faiss.SearchParameters(sel=selector)\n",
    "        _, I_vector = index.search(query_vec, search_k, params=params)\n",
    "    else:\n",
    "        if search_k > 0:\n",
    "            _, I_vector = index.search(query_vec, search_k)\n",
    "        else:\n",
    "            I_vector = [[]]\n",
    "    \n",
    "    vector_indices = I_vector[0].tolist()\n",
    "\n",
    "    # BM25 Retrieval\n",
    "    tokenized_query = technical_tokenizer(user_query)\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    if allowed_indices is not None:\n",
    "        mask = np.zeros_like(bm25_scores)\n",
    "        mask[allowed_indices] = 1\n",
    "        bm25_scores = bm25_scores * mask\n",
    "\n",
    "    bm25_indices = np.argsort(bm25_scores)[::-1][:k * 2].tolist()\n",
    "\n",
    "    # RRF\n",
    "    combined_indices = rrf_fusion(vector_indices, bm25_indices)\n",
    "\n",
    "    # Reranking\n",
    "    candidate_chunks = [final_chunks[i] for i in combined_indices[:10]]\n",
    "    pairs = [[user_query, c['content']] for c in candidate_chunks]\n",
    "    rerank_scores = reranker_model.predict(pairs)\n",
    "    reranked_chunks = [c for _, c in sorted(zip(rerank_scores, candidate_chunks), key=lambda x: x[0], reverse=True)]\n",
    "    \n",
    "    # context expansion\n",
    "    top_chunks = reranked_chunks[:rerank_top_n]\n",
    "    expanded_texts = [get_expanded_content(hit, global_maps, window_size=1) for hit in top_chunks]\n",
    "    context_str = \"\\n\\n \".join(expanded_texts)\n",
    "    full_prompt = f\"You are a helpful agent, answer the question based on provided contexts. Please prioritize the most relevant information, ignore any noise, and answer the question accurately.\\n\\nContext:\\n{context_str}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "\n",
    "    return full_prompt, expanded_texts\n",
    "\n",
    "def query_rag_final_v4(user_query, k=30):\n",
    "    full_prompt, expanded_texts = query_rag_v4(user_query, k, rerank_top_n=3) \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model_llm.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_llm.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return answer, expanded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e148c67-c2dc-42fe-999e-d9fb30a44b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际识别到的列名有: ['Source File', 'Question', 'Ground Truth ']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a935d0a4c524de98ddee75b9bef8e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "questions = []\n",
    "with open('../data/raw/questions.txt', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    reader.fieldnames = [name.strip() for name in reader.fieldnames]\n",
    "    \n",
    "    for row in reader:\n",
    "        q = row.get('Question', '').strip()\n",
    "        gt = row.get('Ground Truth', '').strip()\n",
    "        \n",
    "        if q:\n",
    "            questions.append({\"question\": q, \"ground_truth\": gt})\n",
    "\n",
    "results = []\n",
    "for item in tqdm(questions):\n",
    "    q = item['question']\n",
    "    gt = item['ground_truth']\n",
    "    \n",
    "    ans, contexts = query_rag_final_v4(q)\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": ans,\n",
    "        \"contexts\": contexts,  \n",
    "        \"ground_truth\": gt\n",
    "    })\n",
    "\n",
    "\n",
    "with open(\"../data/eval/results_v4.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "del model_llm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ce76aa-8959-4ce4-8764-bd72069dfc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd1c3290b604397893cdccea2a3c530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@k': np.float64(0.3469387755102041), 'MRR': np.float64(0.7534013605442177), 'Avg Context-GT Sim': np.float64(0.7199309381486957), 'Avg Answer-GT Sim': np.float64(0.7702295959114209), 'Avg Answer-Context Sim': np.float64(0.8472616561740863), 'Hallucination Rate': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embed(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    return test_embeddings.embed_documents(texts)\n",
    "\n",
    "def recall_at_k(contexts, gt, threshold=0.75):\n",
    "    gt_emb = embed(gt)[0]\n",
    "    ctx_embs = embed(contexts)\n",
    "    sims = cosine_similarity([gt_emb], ctx_embs)[0]\n",
    "    hit = np.max(sims) > threshold\n",
    "    best_rank = int(np.argmax(sims)) + 1\n",
    "    return hit, best_rank, float(np.max(sims))\n",
    "\n",
    "def answer_gt_similarity(ans, gt):\n",
    "    emb = embed([ans, gt])\n",
    "    sim = cosine_similarity([emb[0]], [emb[1]])[0][0]\n",
    "    return float(sim)\n",
    "    \n",
    "def answer_context_similarity(ans, contexts):\n",
    "    ans_emb = embed(ans)[0]\n",
    "    ctx_embs = embed(contexts)\n",
    "    sims = cosine_similarity([ans_emb], ctx_embs)[0]\n",
    "    return float(np.max(sims))\n",
    "\n",
    "def hallucination_flag(ans_gt_sim, ans_ctx_sim,\n",
    "                       gt_th=0.6, ctx_th=0.6):\n",
    "    if ans_gt_sim < gt_th and ans_ctx_sim < ctx_th:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "with open(\"../data/eval/results_v4.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for item in tqdm(results):\n",
    "    q = item[\"question\"]\n",
    "    ans = item[\"answer\"]\n",
    "    contexts = item[\"contexts\"]\n",
    "    gt = item[\"ground_truth\"]\n",
    "\n",
    "    hit, rank, ctx_gt_sim = recall_at_k(contexts, gt)\n",
    "    ans_gt_sim = answer_gt_similarity(ans, gt)\n",
    "    ans_ctx_sim = answer_context_similarity(ans, contexts)\n",
    "    hallucinated = hallucination_flag(ans_gt_sim, ans_ctx_sim)\n",
    "\n",
    "    eval_results.append({\n",
    "        \"question\": q,\n",
    "        \"hit\": hit,\n",
    "        \"best_rank\": rank,\n",
    "        \"ctx_gt_sim\": ctx_gt_sim,\n",
    "        \"ans_gt_sim\": ans_gt_sim,\n",
    "        \"ans_ctx_sim\": ans_ctx_sim,\n",
    "        \"hallucinated\": hallucinated\n",
    "    })\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(eval_results)\n",
    "\n",
    "summary = {\n",
    "    \"Recall@k\": df[\"hit\"].mean(),\n",
    "    \"MRR\": (1 / df[\"best_rank\"]).mean(),\n",
    "    \"Avg Context-GT Sim\": df[\"ctx_gt_sim\"].mean(),\n",
    "    \"Avg Answer-GT Sim\": df[\"ans_gt_sim\"].mean(),\n",
    "    \"Avg Answer-Context Sim\": df[\"ans_ctx_sim\"].mean(),\n",
    "    \"Hallucination Rate\": df[\"hallucinated\"].mean()\n",
    "}\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(\"../data/eval/summary_v4.json\", \"w\") as f:\n",
    "    json.dump(summary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
